defaults:
  - training_stages: # can create other training stages in folder and add here
      - s1_motion_estimation
      - s2_motion_compression
      - s3_motion_compensation
      - s4_total_2frame
      - s5_total

# data configs
data:
#  data_dir: /scratch/zczqyc4/vimeo_septuplet # root directory for videos
  data_dir: /scratch/zczqyc4/360-videos-grouped # root directory for videos
  num_workers: 4 # number of dataloader workers
  image_size: [64, 128]
  train_batch_size: 16
  val_batch_size: 16

# resource configs
#ngpu: [4] # number of GPUs
ngpu: [1] # index of GPUs
trainer: # arguments for PTL trainer
#  gpus: 0
#  gpus: [3]
  gpus: ${ngpu}
#  accelerator: ddp

# model config
model: # see DVC for parameter details.
  coder_channels: 128

# PyTorch Lightning module config
module:
  pretrained_model_name: "bmshj2018-hyperprior" # pretrained CompressAI model
  pretrained_model_quality_level: 3 # hyperprior quality level
  distortion_type: "MSE"
  distortion_lambda: 2048.0
  aux_learning_rate: 1e-3 # learning rate for calculating quantiles
  on_sphere_learning: False # whether to use on-sphere learning

# logging configs
checkpoint:
  overwrite: False # overwrite logs already in training dir
  resume_training: True # resume training from previous logs
  model_checkpoint: # passed to PyTorch Lightning's ModelCheckpoint callback
    save_top_k: 1
    monitor: "val_loss"
    save_last: True

# If you want to use previous checkpoints, you need to specify absolute path here.
# If not, use "." as save_root, then for every run, wandb will create a new logging directory and use it as the save_root under dir "outputs/" (e.g. "outputs/2021-07-11/23-19-35/").
logging:
  image_seed: 123 # seed for images to log to wandb
  save_root: "/home/zczqyc4/NeuralCompression/projects/deep_video_compression/outputs/2023-09-03/22-26-09/"
#  save_root: "." # use root of Hydra outputs dir
  num_log_images: 6 # number of images to log to wandb
